from datasets.utils import concatenate_columns, load_from_path, jsonify_columns
import anthropic
import json
import re
import pandas as pd

from argparse import ArgumentParser
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.prompts import (
    FewShotChatMessagePromptTemplate,
    ChatPromptTemplate,
)
from tqdm import tqdm

load_dotenv()

DELIMITER = "\t"
CHUNK_SIZE = 3

TEMPLATE = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë²ˆì—­ê°€ë¡œì„œ ì˜ì–´ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  ìœ¤ë¬¸í•´ì•¼ í•©ë‹ˆë‹¤. ì›ë¬¸ì€ ë¬¸ì¥ ì¸ë±ìŠ¤ë³„ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ ì„ íƒì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\
ì§ˆë¬¸ê³¼ ë‹µë³€ ì„ íƒì§€ëŠ” \tìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë©°, ì²« valueëŠ” ì§ˆë¬¸ì´ê³  ë‚˜ë¨¸ì§€ valueëŠ” ë‹µë³€ ì„ íƒì§€ì…ë‹ˆë‹¤. ë²ˆì—­ë¬¸ì€ ê° ë¬¸ì¥ì„ ë²ˆì—­í•œ í›„ \të¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.\
ë‹¤ìŒ <guidelines>ì„ ì§€ì¼œ ë²ˆì—­í•˜ì„¸ìš”.

<guidelines>
1. ë¬¸ë§¥ì„ ë°˜ì˜í•˜ì—¬ ë²ˆì—­í•˜ì„¸ìš”. ì—¬ê¸°ì„œ ë¬¸ë§¥ì´ë€ <input>ë‚´ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë¬¸ì¥ ì „ì²´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 'organism'ì€ ë¬¸ë§¥ì— ë”°ë¼ 'ìœ ê¸°ì²´, ìƒëª…ì²´, ìƒëª…, ìœ ê¸°ì  ì¡°ì§ì²´' ë“± ë‹¤ì–‘í•˜ê²Œ ë²ˆì—­ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ë²ˆì—­ë¬¸ì€ í•œêµ­ì–´ ì›ì–´ë¯¼ì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•œêµ­ì¸ ì…ì¥ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ì´ë¤„ì ¸ì•¼ í•©ë‹ˆë‹¤. ì˜ì–´ ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ë˜, ì§ì—­ì´ ì–´ìƒ‰í•  ê²½ìš° ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
3. 10ëŒ€ ì²­ì†Œë…„ë„ ì´í•´í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ ë¬¸ì¥ì„ ìœ¤ë¬¸í•˜ì„¸ìš”. ì˜ì–´ ì›ë¬¸ì˜ í†¤ì€ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
4. ë¬¸í™”ì  ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì—¬ í•œêµ­ì–´ í‘œí˜„ì„ ì„ íƒí•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ 'kick the bucket'ì€ 'ì„¸ìƒì„ ë– ë‚˜ë‹¤'ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
5. ì‚¬ëŒ ì´ë¦„ì€ í•œêµ­ì‹ ì´ë¦„ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. ê¸°ì—… ì´ë¦„ì€ ìœ ì‚¬ ì—…ì¢…ì˜ ê°€ì¥ ìœ ëª…í•œ í•œêµ­ ê¸°ì—…ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. 
6. í•œêµ­ì–´ì˜ ê²©ì‹ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²©ì‹ì²´ë€ ë‹¤ìŒê³¼ ê°™ì´ '~ìˆìŠµë‹ˆë‹¤', '~ë‹ˆë‹¤', '~í• ê¹Œìš”?' ë“±ì˜ ë¬¸ì¥ ëë§ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.
7. ì „ë¬¸ ìš©ì–´ë‚˜ ì–´ë ¤ìš´ ìš©ì–´ëŠ” ì˜ì–´ ì›ë¬¸ ë‹¨ì–´ë¥¼ ì¤‘ê´„í˜¸ ì•ˆì— ë„£ì–´ ë²ˆì—­í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ 'í•­ì •ì‹ ì„± ì•½ë¬¼(Antisychotics)ì€ ...'ì™€ ê°™ì´ í‘œê¸°í•©ë‹ˆë‹¤.
8. ë¬¸ì œ í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•  ë•ŒëŠ” ì •ë‹µì„ ì¶”ë¡ í•˜ê±°ë‚˜ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë§Œ ë²ˆì—­í•˜ê³  ì¶”ê°€ì ì¸ ì„¤ëª…ì´ë‚˜ í•´ì„ì„ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”. ë²ˆì—­ ì‹œ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ì´ ìˆë”ë¼ë„ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ '1/3'ì„ '1ì›” 3ì¼'ë¡œ ë²ˆì—­í•˜ì§€ ë§ê³  '1/3'ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
9. ì›ë¬¸ì˜ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”. ì„ íƒì§€, ì •ë‹µ í‘œì‹œ, json í¬ë§· ë“± ì›ë¬¸ì˜ êµ¬ì¡°ì  ìš”ì†Œë¥¼ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”. json ì–‘ì‹ì„ ì œëŒ€ë¡œ ì§€ì¼œ, \" ì—°ì† ì‚¬ìš©, ì½¤ë§ˆ(,) ì‚¬ìš© ë“±ì„ ì£¼ì˜í•˜ì„¸ìš”.
10. None ë˜ëŠ” nan, null ì€ 'ì—†ìŒ'ìœ¼ë¡œ ë²ˆì—­í•˜ì„¸ìš”. 
</guidelines>

<examples>
{examples}
</examples>"""

TEMPLATE_RETRY = """Your answer seems to have some issues with <guidelines> and <examples> above in terms of the format and the structure. Look carefully at the uses of quotes or commas and fix them if they would prevent valid json formatting\
Please check the format and the guidelines and provide again the Korean translations **without any explanations** in the right format."""

client = anthropic.Anthropic()


def _complete_anthropic(messages: list[dict], system_message: str = None):

    try:
        response = client.messages.with_raw_response.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=4096,
            messages=messages,
            system=system_message,
        )
        messages = (
            response.parse()
        )  # get the object that `messages.create()` would have returned
        print(response.headers)
    except anthropic.RateLimitError as e:
        print("ğŸš¨ Rate limit exceeded...")
        print(e)

    return messages


def _format_examples(examples: list[dict]):
    input_examples = json.dumps(
        {idx: f"{ex['user']}" for idx, ex in enumerate(examples)}
    )
    output_examples = json.dumps(
        {idx: f"{ex['assistant']}" for idx, ex in enumerate(examples)}
    )

    return "\nshould be translated and structured into\n".join(
        [f"{input_examples}", f"{output_examples}"]
    )


def _format_chunk(chunk: pd.DataFrame, columns) -> dict[int, str]:
    """
    `jsonify_columns` returns:
    {
        "0": {
            "col0": "In which year was the seminal Human Development Report published?",
            "col1": "It was published in 1990."
        },
        "1": {
            "col0": "Sam bought a car from Tesla. The car dealer offered him a 10% discount, and Sam paid $90,000 for the car. How much was the original price of the car?",
            "col1": "Its original price was $110,000.",
            "col2": "Its original price was $100,000."
        }
    }
    """
    chunk_in_format = {}
    # for idx, row in chunk.iterrows():
    # row_data = concatenate_columns(row, columns)
    for idx in chunk.index:
        row = chunk.loc[idx]
        row_data = jsonify_columns(row, columns)
        chunk_in_format[str(idx)] = row_data
    return json.dumps(chunk_in_format)


def _parse_response(response, columns: list) -> dict[int, dict]:
    """Returns
    {
        0: {"column1": "value1", "column2": "value2", "column3": "value3"},
    }
    """
    parsed = {}
    try:
        response = json.loads(response.content[0].text)
        for idx, row_data in response.items():
            parsed[idx] = {col: str(row_data[col]) for col in columns}
            # values = row_data.split(DELIMITER)
            # parsed[idx] = {col: val for col, val in zip(columns, values)}

    except:
        # can encounter SyntaxError when `eval`ing the response
        return None

    return parsed


def translate_df(df, columns, prompt, path):
    instruct_prompt = prompt.lstrip("Human: ")
    chunks = [df.iloc[i : i + CHUNK_SIZE] for i in range(0, df.shape[0], CHUNK_SIZE)]

    with open(path, "a") as f:
        print(f"Translating {path.name}...")
        if f.tell() == 0:
            f.write(",".join(["id"] + columns) + "\n")

        for chunk in tqdm(chunks):
            formatted_rows = _format_chunk(chunk, columns)
            response = _complete_anthropic(
                messages=[{"role": "user", "content": formatted_rows}],
                system_message=instruct_prompt,
            )
            parsed = _parse_response(response, columns)
            if parsed is None:
                # Try in case of SyntaxError
                response = _complete_anthropic(
                    messages=[
                        {"role": "user", "content": formatted_rows},
                        {"role": "assistant", "content": response.content[0].text},
                        {"role": "user", "content": TEMPLATE_RETRY},
                    ],
                    system_message=instruct_prompt,
                )
                parsed = _parse_response(response, columns)
                if parsed is None:
                    parsed = {
                        idx: {col: response.content[0].text for col in columns}
                        for idx in chunk.index
                    }

            pd.DataFrame(parsed).T.to_csv(f, header=False, index=True)


def main(path, *, columns, headless, with_index, start_index):
    _examples = [
        {
            "user": {
                "col0": "In which year was the seminal Human Development Report published?",
                "col1": "It was published in 1990.",
            },
            "assistant": {
                "col0": "í° ì˜í–¥ë ¥ì„ ê°–ê²Œ ë˜ì—ˆë˜ ì¸ê°„ ê°œë°œ ë³´ê³ ì„œ(Human Development Report)ëŠ” ëª‡ ë…„ë„ì— ë°œí–‰ë˜ì—ˆë‚˜ìš”?",
                "col1": "ë³´ê³ ì„œëŠ” 1990ë…„ì— ë°œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤.",
            },
        },
        {
            "user": {
                "col0": "Sam bought a car from Tesla. The car dealer offered him a 10% discount, and Sam paid $90,000 for the car. How much was the original price of the car?",
                "col1": "Its original price was $110,000.",
                "col2": "Its original price was $100,000.",
            },
            "assistant": {
                "col0": "ë¯¼ìˆ˜ëŠ” ê¸°ì•„ì˜ ì°¨ë¥¼ ìƒ€ìŠµë‹ˆë‹¤. ì°¨ íŒë§¤ìëŠ” ê·¸ì—ê²Œ 10% í• ì¸ì„ ì œê³µí–ˆê³ , ë¯¼ìˆ˜ëŠ” ì°¨ë¥¼ $90,000ì— ìƒ€ìŠµë‹ˆë‹¤. ì°¨ì˜ ì›ë˜ íŒë§¤ê°€ëŠ” ì–¼ë§ˆì˜€ë‚˜ìš”?",
                "col1": "ì°¨ì˜ íŒë§¤ê°€ëŠ” $110,000ì´ì—ˆìŠµë‹ˆë‹¤.",
                "col2": "ì°¨ì˜ íŒë§¤ê°€ëŠ” $100,000ì´ì—ˆìŠµë‹ˆë‹¤.",
            },
        },
    ]

    # _examples = [
    #     {
    #         "input": DELIMITER.join(
    #             [
    #                 "In which year was the seminal Human Development Report published?",
    #                 "It was published in 1990.",
    #             ]
    #         ),
    #         "output": DELIMITER.join(
    #             [
    #                 "ì¤‘ìš”í•œ ì¸ê°„ ê°œë°œ ë³´ê³ ì„œ(Human Development Report)ëŠ” ëª‡ ë…„ë„ì— ë°œí–‰ë˜ì—ˆë‚˜ìš”?",
    #                 "ë³´ê³ ì„œëŠ” 1990ë…„ì— ë°œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤.",
    #             ]
    #         ),
    #     },
    #     {
    #         "input": DELIMITER.join(
    #             [
    #                 "Sam wants to go to bed.",
    #                 "Tesla makes the coolest car in the world.",
    #             ]
    #         ),
    #         "output": DELIMITER.join(
    #             [
    #                 "ë¯¼í˜¸ëŠ” ìë ¤ê³  í•©ë‹ˆë‹¤.",
    #                 "ê¸°ì•„ëŠ” ì„¸ìƒì—ì„œ ê°€ì¥ ë©‹ì§„ ì°¨ë¥¼ ë§Œë“­ë‹ˆë‹¤.",
    #             ]
    #         ),
    #     },
    # ]
    # _example_[pr]ompt = ChatPromptTemplate.from_messages(
    #     [("user", "{input}"), ("assistant", "{output}")]  # ğŸš¨ Anthropic: user/assistant
    # )

    # _few_shot_prompt = FewShotChatMessagePromptTemplate(
    #     examples=_examples,
    #     example_prompt=_example_prompt,
    # )

    _few_shot_prompt = _format_examples(_examples)
    instruct_template = ChatPromptTemplate.from_template(TEMPLATE)
    instruct_prompt = instruct_template.format(examples=_few_shot_prompt)

    for p in tqdm(load_from_path(path)):
        if p.suffix == ".csv":
            df = pd.read_csv(
                p,
                header=None if headless else 0,
                index_col=0 if with_index else None,
            )
        elif p.suffix == ".xlsx":
            df = pd.read_excel(
                p,
                header=None if headless else 0,
                index_col=0 if with_index else None,
            )
        if headless:
            df.columns = [f"col{col}" for col in df.columns]
        if columns:
            df = df[columns]
        if start_index:
            df = df.loc[start_index:]
        columns = df.columns.tolist()  # use all the columns

        translate_df(
            df,
            columns,
            instruct_prompt,
            (
                p.with_suffix(f".from_{start_index}.translated.csv")
                if start_index
                else p.with_suffix(".translated.csv")
            ),
        )

        # # fill in missing columns
        # new_df = new_df.combine_first(df)
        # new_df.to_csv(p.with_suffix(".translated.csv"), index=False)


# chain = final_prompt | llm
# output = chain.invoke(
#     {
#         "input": "/t".join(
#             [
#                 "The quick brown fox jumps over the lazy dog.",
#                 "The passage was coined by Noam Chomsky.",
#             ]
#         ),
#     }
# )

if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("path", type=str)
    parser.add_argument("--headless", action="store_true")
    parser.add_argument("--with-index", action="store_true")
    parser.add_argument("--columns", nargs="+")
    parser.add_argument("--start-index", type=int)
    args = parser.parse_args()
    output = main(
        path=args.path,
        columns=args.columns,
        headless=args.headless,
        with_index=args.with_index,
        start_index=args.start_index,
    )
